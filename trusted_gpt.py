# -*- coding: utf-8 -*-
import os
import json
import concurrent.futures
from model.openai_api import Openai
from model.base_model_api import generation_result
from model.base_model_api import show_model_list
import model.config as config
from datetime import datetime
import template
import importlib.util
import argparse
import random
import re
from postprocessor import structure_response_into_data

import concurrent.futures  # <--- åœ¨è¿™é‡Œæ·»åŠ è¿™ä¸€è¡Œ

class DatasetLoader:
    def __init__(self, file_path, sample_size):
        """
        åˆå§‹åŒ– DatasetLoaderï¼Œæ¥å—æ–‡ä»¶è·¯å¾„ä½œä¸ºå‚æ•°
        :param file_path: æ•°æ®é›†æ–‡ä»¶çš„è·¯å¾„
        :param sample_size: é‡‡æ ·æ•°é‡
        """
        self.file_path = file_path
        self.sample_size = sample_size
        self.data = []

    def load_data(self):
        """
        è¯»å–æŒ‡å®šè·¯å¾„çš„jsonæ–‡ä»¶å¹¶è§£æå†…å®¹
        """
        if not os.path.exists(self.file_path):
            raise FileNotFoundError(f"æ–‡ä»¶è·¯å¾„ä¸å­˜åœ¨: {self.file_path}")

        if self.file_path.endswith('.json'):
            with open(self.file_path, 'r', encoding='utf-8') as f:
                try:
                    file_data = json.load(f)
                    if isinstance(file_data, list):
                        if self.sample_size == -1:
                            self.data = file_data  # é»˜è®¤è¯„æµ‹å…¨éƒ¨
                        else:
                            self.data = random.sample(file_data, self.sample_size)  # éšæœºé‡‡æ ·
                    else:
                        raise ValueError(f"æ–‡ä»¶æ ¼å¼é”™è¯¯: {self.file_path}, å†…å®¹åº”ä¸ºåˆ—è¡¨")
                except json.JSONDecodeError as e:
                    raise ValueError(f"JSONè§£æé”™è¯¯: {self.file_path}, é”™è¯¯ä¿¡æ¯: {str(e)}")
        else:
            raise ValueError(f"æ–‡ä»¶ä¸æ˜¯ä¸€ä¸ªJSONæ–‡ä»¶: {self.file_path}")

    def get_data(self):
        """
        è¿”å›å·²åŠ è½½çš„æ•°æ®
        :return: è§£æåçš„æ•°æ®åˆ—è¡¨
        """
        return self.data


class Task:
    def __init__(self, task_name):
        """
        åˆå§‹åŒ– Task ç±»ï¼Œæ¥å—ä»»åŠ¡åç§°ä½œä¸ºå‚æ•°
        :param task_name: ä»»åŠ¡åç§°ï¼Œç”¨äºåŒ¹é…ä»»åŠ¡çš„ instruction
        """
        self.task_name = task_name
        self.instruction = None
        self.confusion_map = {
            "â‰ˆ": "is approximately equal to",
            ">?": "might be greater than",
            "<?": "might be less than",
            "~": "is related to",
            "?": "is possibly related to",
            "=>": "could imply",
            "<-": "could be derived from",
            "??": "the relationship is unclear",
            "...": "somehow results in",
            "âˆ": "is possibly proportional to",
            "<=>?": "is perhaps equivalent to"
        }

        # å®šä¹‰ä»»åŠ¡æŒ‡ä»¤å­—å…¸ï¼Œå¾®åˆ†ã€ç§¯åˆ†ã€çŸ©é˜µã€å‘é‡ã€å¤æ•°ã€æ¦‚ç‡ã€æé™ã€çº§æ•°æ±‚å’Œã€å¯¹æ•°ã€æŒ‡æ•°ã€å››èˆäº”å…¥ã€ä¸‰è§’å‡½æ•°ã€æ’åˆ—ç»„åˆã€æ¦‚ç‡
        self.task_instructions = {# å’Œè¯»å–çš„æ–‡ä»¶ç›¸å¯¹åº”
            # "UselessCondition": "è¯·ç”¨å˜é‡ aã€bã€c å„å‡ºç°è‡³å°‘ä¸€æ¬¡ï¼Œè®¾è®¡ä¸‰ä¸ªç»“æ„å’Œè¿ç®—å½¢å¼å°½é‡ä¸åŒçš„æ•°å­¦å…¬å¼(å¿…é¡»æ˜¯ç­‰å¼)ã€‚å…¬å¼å¯ä»¥åŒ…å«åŠ æ³•ã€å‡æ³•ã€ä¹˜æ³•ã€é™¤æ³•æˆ–æ‹¬å·è¿ç®—,é‡Œé¢è¦åŒ…å«ä»»æ„çš„å¸¸æ•°ã€‚ä¸‰ä¸ªå…¬å¼ä¹‹é—´å¿…é¡»ç”¨â€œ#â€éš”å¼€ï¼Œè¯·ç›´æ¥å›å¤æ–°å¢çš„ä¸‰ä¸ªå…¬å¼ï¼Œä¸è¦æ·»åŠ ä»»ä½•é¢å¤–å†…å®¹ï¼š",
            "UselessCondition": "Generate three distinct mathematical equations following these strict rules:Content: Each equation must contain the variables a, b, and c. The equations should have diverse structures, using a mix of operations (+, -, *, /), parentheses, and must contain constants.Output Format: You must output all three equations in a single line, separated only by the # symbol.Do not use any numbering, bullet points, or any introductory text. The output must be nothing but the three equations and their separators:",
            # "ConfusedCondition": "è¯·è®¾è®¡ä¸¤ä¸ªå…·æœ‰ç¬¦å·è¿·æƒ‘æ€§çš„æ•°å­¦æ¡ä»¶(å¿…é¡»æ˜¯ç­‰å¼)ï¼Œè¦æ±‚ä½¿ç”¨å˜é‡ xã€yã€zï¼Œæ¡ä»¶ä¸­å¿…é¡»ä½¿ç”¨æ¨¡ç³Šæˆ–ä¸æ˜ç¡®çš„æ•°å­¦ç¬¦å·ï¼Œä»è¿™é‡Œé€‰æ‹©ï¼ˆ\"â‰ˆ\": \"å·®ä¸å¤šæ˜¯\",\">?\": \"è¯´ä¸å®šæ¯”â€¦â€¦å¤§\",\"<?\": \"æˆ–è®¸ä¸å¦‚\",\"?\": \"ä¹Ÿè®¸æœ‰å…³\",\"=>\": \"å¯èƒ½æ„å‘³ç€\",\"->\": \"å¯ä»¥å˜æˆ\",\"<-\": \"å›è¿‡å¤´æ˜¯\",\"??\": \"è¯´ä¸æ¸…æ¥š\",\"...\": \"ç­‰ç­‰\",ï¼‰ã€‚ä¸¤ä¸ªæ¡ä»¶ä¹‹é—´ç”¨â€œ#â€éš”å¼€ï¼Œè¯·ç›´æ¥å›å¤æ–°å¢çš„ä¸¤ä¸ªæ¡ä»¶ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ï¼š",
            "ConfusedCondition": "Please design two mathematical confusing conditions (must be equations). You are required to use the variables x, y, and z. The conditions must use vague or ambiguous mathematical symbols selected from here: (\"â‰ˆ\": \"is approximately equal to\", \">?\": \"might be greater than\", \"<?\": \"might be less than\",\"~\": \"is related to\",\"?\": \"is possibly related to\",\"=>\": \"could imply\",\"<-\": \"could be derived from\",\"??\": \"the relationship is unclear\",\"...\": \"somehow results in\",\"âˆ\": \"is possibly proportional to\",\"<=>?\": \"is perhaps equivalent to\",). You must output the two relationships on a single line.They must be separated only by the # symbol.Do not add any numbering, bullet points, or any text other than the two relationships and the separator:",
            # "FormulaClarifier": "è¯·ä½ æŠŠä¸‹é¢çš„æ¯”è¾ƒç”Ÿç¡¬çš„å˜é‡ä¹‹é—´çš„å…³ç³»è¡¨è¾¾ç¿»è¯‘çš„æ›´åŠ æ¸…æ™°ä¸”ç”ŸåŠ¨ï¼Œç›¸å…³æ•°å­—è¯·ç”¨é˜¿æ‹‰ä¼¯æ•°å­—ï¼Œä¸å¯ä»¥ä½¿ç”¨æ–‡å­—è¡¨è¾¾ï¼ˆä¾‹å¦‚â€œäºŒåå››â€ç­‰ï¼‰ï¼Œä¸è¦å±€é™äºç®€å•è¡¨è¿°ï¼Œå¯ä»¥ä½¿ç”¨æ›´åŠ å¤æ‚çš„è¡¨è¾¾æ–¹å¼ï¼Œä½†æ˜¯æ³¨æ„å˜é‡ä¹‹é—´çš„å…³ç³»å¿…é¡»è¦ä¿æŒå®Œå…¨ä¸å˜ï¼Œè¯·åŠ¡å¿…æ³¨æ„æ¯ä¸ªç³»æ•°æˆ–è€…ç¬¦å·ï¼Œç»å¯¹ä¸å¯ä»¥å¿½ç•¥ä»»ä½•ä¸€ä¸ªç»†èŠ‚ï¼Œä¹Ÿä¸è¦éšæ„æ·»åŠ å˜é‡ç­‰å†…å®¹ï¼ŒåŠ¡å¿…ç¡®ä¿æ¶¦è‰²åä»å…·æœ‰ç²¾ç¡®æ€§ï¼Œæ¯ä¸ªæ¡ä»¶ä¹‹é—´ç”¨å¥å·åˆ†éš”å¼€ï¼Œä¸è¦æ ‡æ³¨æ¡ä»¶çš„åºå·ï¼Œä¸è¦è¾“å‡ºå…¶ä»–ä»»ä½•å†…å®¹ï¼Œå†…å®¹å¦‚ä¸‹ï¼š",
            "FormulaClarifier": "Please translate the following mathematical formulas into natural English descriptions. The goal is to be descriptive and precise, using varied sentence structures while ensuring the mathematical relationships remain unchanged.CRITICAL RULES:Arabic Numerals ONLY: This is the most important rule. All numbers must be written as digits (e.g., 24, -5, 0.5). Never write numbers as words (e.g., \"twenty-four\").Accuracy is Paramount: Pay close attention to every coefficient and symbol, especially negative signs. Do not add or omit any information.No Parentheses: Do not output the parentheses that identify variables.One Condition, One Sentence: Separate each translated condition with a period. Do not number them.Your output must ONLY be the translated sentences.Now, translate the following content following all rules precisely:",
            # "MisleadingCondition": "æˆ‘ç°åœ¨æƒ³ä¸ºä¸€é“é¢˜ç›®æ·»åŠ ä¸¤ä¸ªè¯¯å¯¼æ€§çš„æ¡ä»¶ï¼Œè¯·æ ¹æ®ä»¥ä¸‹å®ä½“å’Œç›¸å…³çš„åŒä¸»é¢˜å®ä½“ï¼Œç”Ÿæˆä¸¤ä¸ªåœ¨é€»è¾‘ä¸Šå«ç³Šä½†ä¸æ”¹å˜è§£ç­”è·¯å¾„çš„å¹²æ‰°æ¡ä»¶ï¼šä½¿ç”¨â€œå®ƒâ€â€œè¿™ä¸ªâ€ä»£è¯æŒ‡ä»£æŸä¸ªç‰¹å®šå¯¹è±¡ï¼Œä½†åœ¨å‰æ–‡ä¸­æåˆ°çš„å¤šä¸ªå®ä½“ä¹‹é—´æ²¡æœ‰æ˜ç¡®æ ‡æ˜æŒ‡ä»£å¯¹è±¡ã€‚åˆ›å»ºä¸¤ä¸ªæ•°å­—æ¡ä»¶ï¼Œé€šè¿‡ä½¿ç”¨æ—¶é—´ã€é¡ºåºæç¤ºè¯­ç­‰ï¼Œå°†å®ƒä»¬è¡¨é¢è¿æ¥èµ·æ¥ï¼Œä½†å®ƒä»¬çš„é€»è¾‘å¹¶ä¸ç›¸å…³ã€‚ç¡®ä¿è¿™äº›æ¡ä»¶åœ¨é˜…è¯»æ—¶èƒ½å¤Ÿå¼•èµ·æ€è€ƒï¼Œä½†ä¸éœ€è¦ç‰¹åˆ«æ¨ç¿»é¢˜ç›®æœ¬èº«çš„è§£ç­”è¿‡ç¨‹ï¼Œä¾‹å¦‚ä¸è¦æ˜ç¡®æŒ‡å‡ºé¢˜ç›®ç›¸å…³çš„å®ä½“ä¹‹é—´çš„ä»£æ•°å…³ç³»ã€‚å¢åŠ ä¸€äº›ç»†èŠ‚ï¼Œè®©è§£ç­”è€…æ³¨æ„ç»†èŠ‚ä½†ä¸è‡³äºäº§ç”Ÿæ— æ³•è§£ç­”çš„æ··ä¹±ã€‚è¯·ä»…è¾“å‡ºä¸¤ä¸ªæ–°å¢æ¡ä»¶ï¼Œæ¯ä¸ªæ¡ä»¶ä¹‹é—´ç”¨â€œ#â€åˆ†éš”ï¼Œä¸æ·»åŠ ä»»ä½•é¢å¤–è¯´æ˜ï¼Œä¹Ÿä¸éœ€è¦åŠ é¡ºåºæ ‡å·ã€‚",
            "MisleadingCondition": "I want to add two misleading conditions to a problem. Based on the provided entities and other related entities within the same topic, generate two distractor conditions that are logically ambiguous but do not alter the solution path: Use a pronoun such as \"it\" or \"this\" to refer to a specific object, but its antecedent is not clearly specified among the multiple entities mentioned earlier. Create two numerical conditions and superficially link them using temporal or sequential cues (e.g., 'then', 'afterwards'), but they are not logically related. Ensure these conditions provoke thought upon reading but do not fundamentally alter the original solution process for the problem; for instance, do not explicitly state an algebraic relationship between the relevant entities. Incorporate details that draw the solver's attention but do not create unsolvable confusion. Strict Output Format:Output only the two new distractor conditions.Separate them with a single # symbol.Do not include any numbering, explanations, or other text:",
            # "ContextGen": "æ¥ä¸‹æ¥è¯·ä½ å¸®æˆ‘è¾“å‡ºä¸‹é¢è¿™ä¸ªæ•°å­¦é¢˜çš„èƒŒæ™¯ä¿¡æ¯ï¼Œè¦æ±‚ç¬¦åˆé€»è¾‘ï¼Œå­—æ•°åœ¨100å­—ä»¥å†…ï¼Œè¯·ç›´æ¥å›å¤æ–°å¢çš„èƒŒæ™¯ä¿¡æ¯ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ï¼š",
            "ContextGen": "Please provide the background information for the following math problem. The information must be logical and under 100 words. Please reply directly with only the new background information and do not add any other content:",
            # "AddCondition": "è¯·ç”Ÿæˆ3ä¸ªæ•°å­¦é¢˜çš„éšæœºæ¡ä»¶ï¼ˆæ¯ä¸ªæ¡ä»¶ä¹‹é—´ç”¨â€œ#â€åˆ†éš”ï¼‰ã€‚æ¡ä»¶éœ€è¦åœ¨ä¸åŒçš„å¤æ‚ç§‘å¹»åœºæ™¯ä¸­ï¼Œä½†ä¸è¦ç»™å‡ºé—®é¢˜ã€‚åªéœ€è¦æ¡ä»¶ï¼Œä¸è¦äº§ç”Ÿä»»ä½•é¢å¤–è¾“å‡ºï¼Œä¹Ÿä¸éœ€è¦åŠ é¡ºåºæ ‡å·",
            "AddCondition": "Please generate 3 random conditions for math problems (separate each condition with a \"#\"). The conditions need to be set in different, complex science-fiction scenarios, but do not provide a question. Only the conditions are needed, do not produce any extra output, and do not add sequential numbering.",
        }
        # å˜å¼‚

        # è·å–å¯¹åº”çš„ä»»åŠ¡æŒ‡ä»¤
        self._match_task()

    def _match_task(self):
        """
        åŒ¹é…ä»»åŠ¡æŒ‡ä»¤å¹¶å­˜å‚¨åˆ°ç±»å±æ€§ä¸­
        """
        self.instruction = self.task_instructions.get(self.task_name)
        if not self.instruction:
            raise ValueError(f"ä»»åŠ¡ {self.task_name} æœªæ‰¾åˆ°å¯¹åº”çš„ instruction")

    def get_instruction(self):
        """
        è·å–ä»»åŠ¡çš„æŒ‡ä»¤
        :return: ä»»åŠ¡æŒ‡ä»¤å­—ç¬¦ä¸²
        """
        return self.instruction


class Inferrence:
    def __init__(self, data, task_obj, input_path, output_path,task_name):
        """
        åˆå§‹åŒ– Inferrence ç±»
        :param data: ä»»åŠ¡æ•°æ®åˆ—è¡¨
        :param task_obj: å®Œæ•´çš„ Task å¯¹è±¡ <--- ä¿®æ”¹ç‚¹
        :param input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
        :param output_path: è¾“å‡ºæ–‡ä»¶ä¿å­˜è·¯å¾„
        """
        self.data = data
        self.task_obj = task_obj  # <-- å­˜å‚¨æ•´ä¸ª task å¯¹è±¡
        self.instruction = task_obj.get_instruction() # <-- instruction å¯ä»¥ä» task å¯¹è±¡ä¸­è·å–
        self.input_path = input_path
        self.output_path = output_path
        self.task = task_name # <-- ä½¿ç”¨æ–°çš„å‚æ•°å
        self.model_classes = {}  # å­˜å‚¨æ¨¡å‹ç±»
        self.model_instances = {}  # å­˜å‚¨æ¨¡å‹å®ä¾‹
        self.failed_models = []  # å­˜å‚¨æœªæˆåŠŸåŠ è½½æˆ–å®ä¾‹åŒ–çš„æ¨¡å‹åç§°
    def load_models(self):
        """
        åŠ¨æ€åŠ è½½æ”¯æŒçš„æ¨¡å‹ç±»ï¼Œå¹¶å®ä¾‹åŒ–æ¨¡å‹å¯¹è±¡
        """
        # åŠ¨æ€å¯¼å…¥æ‰€æœ‰æ”¯æŒçš„æ¨¡å‹ç±»
        total_model_list = os.listdir("./model")
        for model in total_model_list:
            if "api" in model and "base" not in model:
                model_name = model.split("_")[0].capitalize()
                model_path = model.split(".")[0]
                execute_command = f"""
from model.{model_path} import {model_name}
self.model_classes["{model_name}"] = {model_name}
                """
                try:
                    exec(execute_command)
                except Exception as e:
                    print(f"Error loading model {model_name}: {e}")


         # å®ä¾‹åŒ–æ¨¡å‹å¯¹è±¡
        for model_name, model_class in self.model_classes.items():
            try:
                api_keys = getattr(config, f"{model_name.lower()}_api_keys", None)
                if not api_keys:
                    raise ValueError(f"API keys for {model_name} not found in config.")

                model_version = model_class.MOST_RECOMMENDED_MODEL[0]
                # model_version = model_class.SOTA
                model_instance = model_class(api_keys, model_version)
                self.model_instances[model_name] = model_instance
            except Exception as e:
                print(f"Error instantiating model {model_name}: {e}")
                self.failed_models.append(model_name)

        if self.failed_models:
            print(f"Models that failed to load or instantiate: {self.failed_models}")

                # æ‰“å°æˆåŠŸå®ä¾‹åŒ–çš„æ¨¡å‹åˆ—è¡¨
        if self.model_instances:
            print("Successfully instantiated models:")
            for model_name in self.model_instances:
                print(f" - {model_name}")
        else:
            print("No models were successfully instantiated.")


    def generate_prompts(self):
        """
        æ ¹æ®ä»»åŠ¡æ•°æ®ç”Ÿæˆ prompts
        :return: ç”Ÿæˆçš„ prompts åˆ—è¡¨
        """
        prompts = []

        if self.task == "FormulaClarifier":
            # ä» task å¯¹è±¡ä¸­è·å–åŸºç¡€æŒ‡ä»¤å’Œç¬¦å·æ˜ å°„
            base_instruction = self.instruction
            confusion_map = self.task_obj.confusion_map

            # å°†ç¬¦å·æ˜ å°„æ ¼å¼åŒ–ä¸ºæ˜“äºé˜…è¯»çš„å­—ç¬¦ä¸²
            confusion_map_str = "\n".join([f'- "{symbol}": "{meaning}"' for symbol, meaning in confusion_map.items()])

            for item in self.data:
                # è·å–å½“å‰æ•°æ®é¡¹çš„ xyzmn_mapping
                xyzmn_mapping = item.get("xyzmn_mapping", {})
                # å°†å˜é‡æ˜ å°„æ ¼å¼åŒ–ä¸ºæ˜“äºé˜…è¯»çš„å­—ç¬¦ä¸²
                xyzmn_mapping_str = "\n".join(
                    [f'- "{variable}": "{meaning}"' for variable, meaning in xyzmn_mapping.items()])

                # è·å–éœ€è¦ç¿»è¯‘çš„åŸå§‹ prompt å†…å®¹
                content_to_translate = item.get("prompt", "")

                # ä½¿ç”¨ f-string æ„å»ºä¸€ä¸ªç»“æ„æ¸…æ™°ã€ä¿¡æ¯ä¸°å¯Œçš„ prompt
                prompt = f"""{base_instruction}

        Here is the context you must use:

        **1. Symbol Meanings:**
        {confusion_map_str}

        **2. Variable Meanings:**
        {xyzmn_mapping_str}

        **Content to Translate:**
        **Do not be limited to simple statements; you may use more complex and obfuscating expressions. The output content is all natural language, rather than translating natural language into mathematical formulas.**
        ---
        {content_to_translate}
        ---
        """
                prompts.append(prompt)
        # å¦‚æœ step == "AddContext"ï¼Œåˆ™ä¸åŒ…å« JSON æ–‡ä»¶ä¸­çš„å†…å®¹
        elif self.task in ["UselessCondition", "ConfusedCondition", "AddCondition"]:
            prompts = [self.instruction] * len(self.data)  # ä»…ä½¿ç”¨ instruction
        elif self.task in ["MisleadingCondition"]:
            for item in self.data:
                # æå– xyzmn_mapping å’Œ abc_mapping
                xyzmn_mapping = item.get("xyzmn_mapping", {})
                abc_mapping = item.get("abc_mapping", {})

                # å°†æ˜ å°„è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼ï¼Œä¾‹å¦‚ï¼š "x: è‹¹æœ, y: æ©˜å­, ..."
                xyzmn_str = ", ".join([f"{value}" for value in xyzmn_mapping.values()])
                abc_str = ", ".join([f"{value}" for value in abc_mapping.values()])
                # ç›´æ¥è¯»å– unmapped_entities
                unmapped_entities = item.get("unmapped_entities", [])
                # å°†æœªæ˜ å°„çš„å®ä½“è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                unmapped_str = ", ".join(unmapped_entities)

                # æ’å…¥æ˜ å°„åˆ°æŒ‡ä»¤ä¸­
                prompt = f"{self.instruction}\n"  # åŸºç¡€æŒ‡ä»¤
                prompt += f"é¢˜ç›®ç›¸å…³çš„å®ä½“ï¼š{xyzmn_str}\n"  # æ’å…¥ xyzmn_mapping
                prompt += f"é¢˜ç›®ä¸­ä¸å­˜åœ¨ä½†æ˜¯ç›¸åŒä¸»é¢˜çš„å®ä½“ï¼š{abc_str}\n"  # æ’å…¥ abc_mapping
                prompt += f"æœªæ˜ å°„çš„å®ä½“ï¼š{unmapped_str}\n"  # æ’å…¥æœªæ˜ å°„å®ä½“
                # prompt += f"{item['prompt']}"  # åŠ å…¥æ•°æ®ä¸­çš„ prompt
                prompts.append(prompt)
        else:
            for item in self.data:
                prompt = f"{self.instruction}\n{item['prompt']}"
                prompts.append(prompt)
        return prompts

    def _process_chunk(self, model_instance, data_chunk, prompts_chunk):
        """
        å¤„ç†å•ä¸ªæ•°æ®å—ï¼šè°ƒç”¨APIå¹¶è¿”å›å¤„ç†åçš„æ•°æ®å’Œtokenç»Ÿè®¡ã€‚
        :param model_instance: è¦ä½¿ç”¨çš„æ¨¡å‹å®ä¾‹ã€‚
        :param data_chunk: å½“å‰è¦å¤„ç†çš„æ•°æ®é¡¹åˆ—è¡¨ã€‚
        :param prompts_chunk: å¯¹åº”çš„æç¤ºåˆ—è¡¨ã€‚
        :return: ä¸€ä¸ªå…ƒç»„ (processed_chunk, prompt_tokens, completion_tokens, successful_calls)
        """
        processed_chunk = []
        prompt_tokens = 0
        completion_tokens = 0
        successful_calls = 0

        try:
            results = generation_result(model_instance, prompts_chunk)
            for i, item in enumerate(data_chunk):
                status, text_response, full_json_response = results[i]

                if status == "success":
                    usage_data = full_json_response.get('usage', {})
                    prompt_tokens += usage_data.get('prompt_tokens', 0)
                    completion_tokens += usage_data.get('completion_tokens', 0)
                    successful_calls += 1

                processed_item = {
                    **item,
                    "response": text_response,
                }
                processed_chunk.append(processed_item)

        except Exception as e:
            # å¦‚æœ generation_result å†…éƒ¨å‡ºé”™ï¼Œä¸ºè¿™ä¸ªå—çš„æ‰€æœ‰é¡¹è®°å½•é”™è¯¯
            print(f"Error processing a chunk: {e}")
            for item in data_chunk:
                processed_item = {
                    **item,
                    "response": f"Error: Failed to process chunk due to {e}",
                }
                processed_chunk.append(processed_item)

        return processed_chunk, prompt_tokens, completion_tokens, successful_calls

    # ^^^^ æ–°çš„è¾…åŠ©å‡½æ•°åˆ°æ­¤ç»“æŸ ^^^^

    def run_inference(self, model_name, step):
        model_instance = self.model_instances.get(model_name)
        if not model_instance:
            print(f"Model {model_name} not instantiated")
            return

        print(f"Running inference for model: {model_name}")
        # 1. åˆå§‹åŒ–æ±‡æ€»å˜é‡
        total_prompt_tokens = 0
        total_completion_tokens = 0
        total_successful_calls = 0
        all_processed_data = []
        # å®šä¹‰å¹¶è¡Œä»»åŠ¡æ•°
        NUM_WORKERS = 15

        # 2. å°†æ•°æ®å’Œæç¤ºåˆ†å‰²æˆ N ä¸ªå—
        # è®¡ç®—æ¯ä¸ªå—çš„å¤§è‡´å¤§å°
        chunk_size = len(self.data) // NUM_WORKERS
        if len(self.data) % NUM_WORKERS != 0:
            chunk_size += 1  # ç¡®ä¿æ‰€æœ‰é¡¹ç›®éƒ½è¢«è¦†ç›–

        data_chunks = [self.data[i:i + chunk_size] for i in range(0, len(self.data), chunk_size)]
        prompts_chunks = [self.prompts[i:i + chunk_size] for i in range(0, len(self.prompts), chunk_size)]

        FUTURE_TIMEOUT = 310
        # 3. åˆ›å»ºçº¿ç¨‹æ± å¹¶æäº¤ä»»åŠ¡
        with concurrent.futures.ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:
            # æäº¤ä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡å¤„ç†ä¸€ä¸ªæ•°æ®å—
            future_to_chunk = {
                executor.submit(self._process_chunk, model_instance, data_chunks[i], prompts_chunks[i]): i for i in
                range(len(data_chunks))}

            print(f"Dispatched {len(data_chunks)} chunks to {NUM_WORKERS} workers.")

            # 4. æ”¶é›†å¹¶æ±‡æ€»ç»“æœ
            for future in concurrent.futures.as_completed(future_to_chunk):
                chunk_index = future_to_chunk[future]
                try:
                    full_response_object = future.result(timeout=FUTURE_TIMEOUT)
                    # ä» _process_chunk çš„è¿”å›ä¸­è§£åŒ…
                    processed_chunk, prompt_tokens, completion_tokens, successful_calls = future.result()

                    all_processed_data.extend(processed_chunk)
                    total_prompt_tokens += prompt_tokens
                    total_completion_tokens += completion_tokens
                    total_successful_calls += successful_calls
                    print(f"Chunk {chunk_index + 1}/{len(data_chunks)} completed.")

                except Exception as exc:
                    print(f'Chunk {chunk_index} generated an exception: {exc}')


        processed_new_data = structure_response_into_data(
            all_processed_data,
            self.input_path,
            separator="#",
            field="prompt",
            task_name=self.task
        )


        # 3. å®šä¹‰ä¸»è¾“å‡ºæ–‡ä»¶çš„è·¯å¾„
        output_file = os.path.join(self.output_path, f"{step}.json")

        # 4. ä»ä¸»è¾“å‡ºæ–‡ä»¶ä¸­ï¼Œè¯»å–ä¹‹å‰å·²ç»æˆåŠŸçš„é¡¹ç›®
        #    æˆ‘ä»¬å¤ç”¨ä¹‹å‰å®šä¹‰çš„ filter_failed_items å‡½æ•°ï¼Œå®ƒèƒ½åŒæ—¶è¿”å›æˆåŠŸå’Œå¤±è´¥çš„åˆ—è¡¨
        _, successful_items = filter_failed_items(output_file)
        if successful_items is None:
            successful_items = []  # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼ˆå³ç¬¬ä¸€æ¬¡è¿è¡Œï¼‰ï¼Œåˆ™æˆåŠŸåˆ—è¡¨ä¸ºç©º

        # 5. å°†ã€å·²ç»æˆåŠŸçš„æ—§é¡¹ç›®ã€‘å’Œã€æœ¬è½®æ–°å¤„ç†å¥½çš„é¡¹ç›®ã€‘åˆå¹¶æˆä¸€ä¸ªå®Œæ•´çš„åˆ—è¡¨
        final_output_data = successful_items + processed_new_data

        if final_output_data:  # ç¡®ä¿åˆ—è¡¨ä¸ä¸ºç©º
            # ä½¿ç”¨æ›´å¥å£®çš„è‡ªç„¶æ’åºï¼Œä»¥ç¡®ä¿IDæ’åºçš„å‡†ç¡®æ€§
            def sort_key(item):
                item_id = item.get('id', '0_0')
                try:
                    # å°è¯•æŒ‰ "æ•°å­—_æ•°å­—" æ ¼å¼è§£æ
                    part1, part2 = map(int, str(item_id).split('_'))
                    return (part1, part2)
                except (ValueError, TypeError):
                    # å¦‚æœå¤±è´¥ï¼Œå°è¯•æŒ‰å•ä¸ªæ•°å­—è§£æ
                    try:
                        return (int(item_id), 0)
                    except (ValueError, TypeError):
                        # å¦‚æœå†æ¬¡å¤±è´¥ï¼Œåˆ™æ’åœ¨æœ€å
                        return (float('inf'), float('inf'))

            final_output_data = sorted(final_output_data, key=sort_key)
        # ^^^^ æ·»åŠ ç»“æŸ ^^^^

        # 6. å°†åˆå¹¶åçš„å®Œæ•´æ•°æ®å†™å…¥æ–‡ä»¶
        model_name_SOTA = model_instance.MOST_RECOMMENDED_MODEL[0]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        task_folder_path = self.output_path

        if not os.path.exists(task_folder_path):
            os.makedirs(task_folder_path)

        output_file1 = os.path.join(task_folder_path, f"{step}_{timestamp}.json")

        # å°†æœ€ç»ˆçš„ã€å®Œæ•´çš„æ•°æ®å†™å…¥ä¸»è¾“å‡ºæ–‡ä»¶å’Œå¸¦æ—¶é—´æˆ³çš„å¤‡ä»½æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(final_output_data, f, ensure_ascii=False, indent=4)

        with open(output_file1, 'w', encoding='utf-8') as f:
            json.dump(final_output_data, f, ensure_ascii=False, indent=4)

        print(f"Model {model_name_SOTA} inference completed successfully for this batch.")
        print(f"Output file '{output_file}' now contains {len(final_output_data)} total items.")
        print("\n--- æœ¬æ¬¡è¿è¡ŒTokenä½¿ç”¨æ€»è®¡ ---")
        print(f"æˆåŠŸçš„APIè°ƒç”¨æ¬¡æ•°: {total_successful_calls}")
        print(f"æ€»è¾“å…¥ Tokens (Prompt): {total_prompt_tokens}")
        print(f"æ€»è¾“å‡º Tokens (Completion): {total_completion_tokens}")
        print(f"æ€»æ¶ˆè€— Tokens: {total_prompt_tokens + total_completion_tokens}")
        print("---------------------------------\n")
        return

    def run_inference_sequential(self):
        """
        æ‰§è¡Œæ‰€æœ‰æ¨¡å‹çš„æ¨ç†ä»»åŠ¡
        """
        # æ‰“å°æˆåŠŸå®ä¾‹åŒ–çš„æ¨¡å‹åˆ—è¡¨
        if self.model_instances:
            print("Successfully instantiated models:")
            for model_name in self.model_instances:
                print(f" - {model_name}")
        else:
            print("No models were successfully instantiated.")
            return

        # ç”Ÿæˆ promptsï¼Œåªåšä¸€æ¬¡
        self.prompts=self.generate_prompts()

        # é€ä¸ªæ¨¡å‹æ‰§è¡Œæ¨ç†ä»»åŠ¡
        for model_name in self.model_instances:
            self.run_inference(model_name)

    def run_inference_on_models(self, model_names, step):
        """
        æ ¹æ®ä¼ å…¥çš„æ¨¡å‹åç§°åˆ—è¡¨ï¼Œä¾æ¬¡æ‰§è¡Œæ¨ç†ä»»åŠ¡
        :param model_names: æ¨¡å‹åç§°åˆ—è¡¨
        """
        # ç”Ÿæˆ promptsï¼Œåªåšä¸€æ¬¡
        self.prompts=self.generate_prompts()
        for model_name in model_names:
            if model_name in self.model_instances:
                print(f"Model {model_name} found. Running inference...")
                self.run_inference(model_name, step)
            else:
                print(f"Model {model_name} not found in instantiated models.")


class Evaluator:
    def __init__(self, output_folder, task, output_path, answer_file=None, eval_mode=False, model_list=None):
        """
        åˆå§‹åŒ– Evaluator ç±»
        :param output_folder: éœ€è¦è¯„ä¼°çš„è¾“å‡ºæ–‡ä»¶æ‰€åœ¨çš„æ–‡ä»¶å¤¹è·¯å¾„
        :param answer_file: å­˜æ”¾æ ‡å‡†ç­”æ¡ˆçš„ JSON æ–‡ä»¶è·¯å¾„
        :param task: ä»»åŠ¡åç§°ï¼Œç”¨äºæŒ‡å®šåŠ è½½å“ªä¸ªè¯„ä¼°è„šæœ¬
        :param output_path: è¯„ä¼°ç»“æœçš„ä¿å­˜è·¯å¾„
        :param eval_mode: æ˜¯å¦ä¸ºè¯„æµ‹æ¨¡å¼
        :param model_list: è¯„æµ‹ä½¿ç”¨çš„æ¨¡å‹åˆ—è¡¨
        """
        self.output_folder = output_folder
        self.answer_file = answer_file
        self.task = task
        self.output_path = output_path
        self.evaluation_script = None
        self.eval_mode = eval_mode
        self.model_list = model_list
        self.model_instances = {}
        self.special_dataset_tags = ["GSM8K"]

        # å¦‚æœæ˜¯è¯„æµ‹æ¨¡å¼ï¼ŒåŠ è½½æ¨¡å‹
        if self.eval_mode:
            self._load_models_for_evaluation()

    def _load_models_for_evaluation(self):
        """
        ä¸ºè¯„æµ‹æ¨¡å¼åŠ è½½æ¨¡å‹ï¼Œä¼šåŠ è½½æ‰€æœ‰æ¨èçš„æ¨¡å‹ç‰ˆæœ¬
        """
        # åŠ¨æ€å¯¼å…¥æ‰€æœ‰æ”¯æŒçš„æ¨¡å‹ç±»
        total_model_list = os.listdir("./model")
        model_classes = {}

        for model in total_model_list:
            if "api" in model and "base" not in model:
                model_name = model.split("_")[0].capitalize()
                model_path = model.split(".")[0]
                execute_command = f"""
from model.{model_path} import {model_name}
model_classes["{model_name}"] = {model_name}
                """
                try:
                    exec(execute_command)
                except Exception as e:
                    print(f"Error loading model {model_name}: {e}")

        # ã€ä¸»è¦ä¿®æ”¹ç‚¹ã€‘å®ä¾‹åŒ–æ¨¡å‹å¯¹è±¡ï¼Œä¸ºæ¯ä¸ªæ¨èç‰ˆæœ¬éƒ½åˆ›å»ºå®ä¾‹
        for model_class_name in self.model_list:
            if model_class_name in model_classes:
                self.model_instances[model_class_name] = {}  # åˆ›å»ºå­å­—å…¸
                try:
                    api_keys = getattr(config, f"{model_class_name.lower()}_api_keys", None)
                    if not api_keys:
                        raise ValueError(f"API keys for {model_class_name} not found in config.")

                    model_class = model_classes[model_class_name]
                    if not hasattr(model_class, 'EVALUATION_MODELS'):
                        print(f"Warning: Model class {model_class_name} does not have an 'EVALUATION_MODELS' list. Skipping for evaluation.")
                        continue
                    # éå†æ‰€æœ‰æ¨èæ¨¡å‹
                    for model_version in model_class.EVALUATION_MODELS:
                        model_instance = model_class(api_keys, model_version)
                        self.model_instances[model_class_name][model_version] = model_instance
                        print(f"Successfully loaded model for evaluation: {model_class_name} - {model_version}")

                except Exception as e:
                    print(f"Error instantiating model {model_class_name} for evaluation: {e}")
            else:
                print(f"Model {model_class_name} not found in available models")

    def _load_evaluation_script(self):
        """
        åŠ¨æ€åŠ è½½è¯„ä¼°è„šæœ¬
        """
        script_path = f"./Evaluator/{self.task}_evaluator.py"
        if not os.path.exists(script_path):
            raise FileNotFoundError(f"è¯„ä¼°è„šæœ¬ {script_path} æœªæ‰¾åˆ°")

        spec = importlib.util.spec_from_file_location(f"{self.task}_evaluator", script_path)
        self.evaluation_script = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(self.evaluation_script)

    def _save_results(self, evaluation_results, model_version_name, task):
        """
        ä¿å­˜è¯„ä¼°ç»“æœåˆ°ä»¥æ¨¡å‹åå‘½åçš„ä¸“å±æ–‡ä»¶å¤¹ä¸­
        """

        # --- æ–°å¢é€»è¾‘ï¼šåŠ¨æ€ç”Ÿæˆæ–‡ä»¶ååç¼€ ---
        filename_suffix = ""
        # éå†æˆ‘ä»¬åœ¨ __init__ ä¸­å®šä¹‰çš„ç‰¹æ®Šæ•°æ®é›†æ ‡ç­¾åˆ—è¡¨
        for tag in self.special_dataset_tags:
            # æ£€æŸ¥è¾“å…¥æ–‡ä»¶è·¯å¾„(self.answer_file)æ˜¯å¦åŒ…å«è¯¥æ ‡ç­¾
            if tag in self.answer_file:
                filename_suffix = f"_{tag}"
                break  # æ‰¾åˆ°ç¬¬ä¸€ä¸ªåŒ¹é…çš„æ ‡ç­¾åå°±åœæ­¢æŸ¥æ‰¾


        model_specific_dir = os.path.join(self.output_path, model_version_name)

        # 2. ã€ä¸»è¦ä¿®æ”¹ç‚¹ã€‘ç¡®ä¿è¿™ä¸ªæ¨¡å‹ä¸“ç”¨çš„ç›®å½•å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å°±åˆ›å»ºå®ƒ
        os.makedirs(model_specific_dir, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        if filename_suffix == '':
            output_file_path = os.path.join(model_specific_dir, f"{model_version_name}_evaluation_results_{timestamp}.json")
            output_file_path1 = os.path.join(model_specific_dir, f"evaluation_results.json")
        else:
            output_file_path = os.path.join(model_specific_dir, f"{model_version_name}_evaluation_results{filename_suffix}_{timestamp}.json")
            output_file_path1 = os.path.join(model_specific_dir, f"evaluation_results{filename_suffix}.json")


        # ä¿å­˜æ–‡ä»¶çš„é€»è¾‘ä¿æŒä¸å˜
        with open(output_file_path, 'w', encoding='utf-8') as f:
            json.dump(evaluation_results, f, ensure_ascii=False, indent=4)
        with open(output_file_path1, 'w', encoding='utf-8') as f:
            json.dump(evaluation_results, f, ensure_ascii=False, indent=4)

        print(f"è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_file_path1}")

    def run_model_evaluation(self, model_instance, data):
        """
        ä½¿ç”¨æŒ‡å®šæ¨¡å‹å®ä¾‹å¯¹æ•°æ®è¿›è¡Œè¯„æµ‹ï¼Œå¹¶å®Œæ•´é€ä¼ åŸå§‹æ•°æ®ã€‚
        """
        prompts = [item["prompt"] for item in data['problems']]
        total_problems = len(prompts)
        print(f"  > æ‰¾åˆ° {total_problems} é“é¢˜ç›®éœ€è¦è¯„æµ‹ã€‚")
        print(f"  > æ­£åœ¨å°†æ‰€æœ‰æç¤ºå‘é€è‡³æ¨¡å‹è¿›è¡Œç”Ÿæˆ...")

        results = generation_result(model_instance, prompts)
        print(f"  > å·²æ”¶åˆ° {len(results)} ä¸ªå›å¤ã€‚æ­£åœ¨å¤„ç†ç»“æœ...")

        evaluation_results = []
        for i, item in enumerate(data['problems']):
            # ã€æ ¸å¿ƒä¿®æ”¹ã€‘
            # 1. ä½¿ç”¨å­—å…¸è§£åŒ… `**item` æ¥å¤åˆ¶åŸå§‹ item çš„æ‰€æœ‰å­—æ®µï¼Œå®ç°æ•°æ®é€ä¼ ã€‚
            # 2. ç„¶åæ·»åŠ æˆ–è¦†ç›– `model_response` å­—æ®µã€‚
            result = {
                **item,  # <-- è¿™ä¸€è¡Œæ˜¯å…³é”®ï¼Œå®ƒä¼šå¤åˆ¶ item ä¸­çš„æ‰€æœ‰é”®å€¼å¯¹
                "model_response": results[i][1]
            }
            evaluation_results.append(result)

        return evaluation_results

    def evaluate(self):
        """
        æ‰§è¡Œè¯„æµ‹æµç¨‹ï¼ŒåŒ…å«å¤±è´¥é‡è¯•æœºåˆ¶ã€‚
        """
        if not self.eval_mode:
            # åŸè¯„ä¼°é€»è¾‘ (éevalæ¨¡å¼ä¿æŒä¸å˜)
            # ... (è¿™éƒ¨åˆ†ä»£ç ä¿æŒåŸæ ·å³å¯)
            evaluation_results = {}
            for output_file in os.listdir(self.output_folder):
                if output_file.endswith('.json') and '_' not in output_file:
                    output_file_path = os.path.join(self.output_folder, output_file)
                    with open(output_file_path, 'r', encoding='utf-8') as f:
                        model_output = json.load(f)

                    evaluator_instance = self.evaluation_script.Evaluator(output_file_path, self.answer_file)
                    evaluation_result = evaluator_instance.evaluate()
                    evaluation_results[output_file] = evaluation_result
            return evaluation_results
        else:
            # --- æ–°çš„ã€å¸¦é‡è¯•åŠŸèƒ½çš„è¯„æµ‹é€»è¾‘ ---
            eval_input_file = self.answer_file
            if not os.path.exists(eval_input_file):
                raise FileNotFoundError(f"æŒ‡å®šçš„è¯„æµ‹è¾“å…¥æ–‡ä»¶æœªæ‰¾åˆ°: {eval_input_file}")

            with open(eval_input_file, 'r', encoding='utf-8') as f:
                loaded_json = json.load(f)

            if isinstance(loaded_json, list):
                # å¦‚æœæ–‡ä»¶å†…å®¹ç›´æ¥æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œå°±æ‰‹åŠ¨å°†å®ƒåŒ…è£…æˆæˆ‘ä»¬éœ€è¦çš„å­—å…¸æ ¼å¼
                initial_data = {'problems': loaded_json}
            else:
                # å¦åˆ™ï¼Œæˆ‘ä»¬å‡å®šå®ƒå·²ç»æ˜¯æ­£ç¡®çš„å­—å…¸æ ¼å¼äº†
                initial_data = loaded_json
            if 'problems' in initial_data and isinstance(initial_data['problems'], list):
                total_num_problems = len(initial_data['problems'])
                print(f"åŸå§‹æ•°æ®æ€»å…±æœ‰ {total_num_problems} æ¡ã€‚")
                sample_count = 50

                # ç¡®ä¿æ ·æœ¬æ•°é‡ä¸è¶…è¿‡æ•°æ®é›†çš„æ€»æ•°
                if total_num_problems > sample_count:
                    random.seed(42)  # 42æ˜¯ä¸€ä¸ªå¸¸ç”¨çš„æƒ¯ä¾‹ï¼Œæ‚¨ä¹Ÿå¯ä»¥æ¢æˆä»»ä½•å…¶ä»–æ•´æ•°
                    # ä½¿ç”¨ random.sample è¿›è¡Œéšæœºé‡‡æ ·
                    initial_data['problems'] = random.sample(initial_data['problems'], sample_count)
                    print(f"å·²ä»æ•°æ®é›†ä¸­éšæœºé€‰æ‹© {len(initial_data['problems'])} æ¡æ•°æ®ç”¨äºæœ¬æ¬¡è¯„æµ‹ã€‚")
                else:
                    # å¦‚æœæ€»æ•°ä¸è¶³100ï¼Œåˆ™ä½¿ç”¨å…¨éƒ¨æ•°æ®
                    print(f"æ•°æ®é›†æ€»æ•°ä¸è¶³{sample_count}æ¡ï¼Œå°†ä½¿ç”¨å…¨éƒ¨ {total_num_problems} æ¡æ•°æ®è¿›è¡Œè¯„æµ‹ã€‚")
            else:
                print("è­¦å‘Šï¼šæ— æ³•åœ¨åŠ è½½çš„æ•°æ®ä¸­æ‰¾åˆ° 'problems' åˆ—è¡¨ï¼Œå°†ç»§ç»­å¤„ç†å…¨éƒ¨æ•°æ®ã€‚")
            # å¯¹æ¯ä¸ªåŠ è½½çš„æ¨¡å‹ç‰ˆæœ¬è¿è¡Œè¯„æµ‹
            for model_class_name, model_versions in self.model_instances.items():
                print(f"--- å¼€å§‹å¤„ç†æ¨¡å‹æ—: {model_class_name} ---")
                for model_version_name, model_instance in model_versions.items():
                    print(f"\n>>> æ­£åœ¨è¯„æµ‹æ¨¡å‹: {model_version_name}")

                    # 0. ç¡®å®šå½“å‰æ¨¡å‹å¯¹åº”çš„è¾“å‡ºæ–‡ä»¶è·¯å¾„
                    # (è¿™éƒ¨åˆ†é€»è¾‘æ¥è‡ª _save_results æ–¹æ³•ï¼Œæˆ‘ä»¬éœ€è¦åœ¨è¿™é‡Œå…ˆç¡®å®šè·¯å¾„)
                    filename_suffix = ""
                    for tag in self.special_dataset_tags:
                        if tag in self.answer_file:
                            filename_suffix = f"_{tag}"
                            break
                    model_specific_dir = os.path.join(self.output_path, model_version_name)
                    os.makedirs(model_specific_dir, exist_ok=True)
                    output_file_path = os.path.join(model_specific_dir, f"evaluation_results{filename_suffix}.json")

                    # --- åœ¨å¾ªç¯å¼€å§‹å‰ï¼Œæ¸…ç©ºæ—§çš„è¾“å‡ºæ–‡ä»¶ (å’Œä½ ä¹‹å‰çš„è¦æ±‚ä¸€è‡´) ---
                    if os.path.exists(output_file_path):
                        os.remove(output_file_path)
                        print(f"å·²åˆ é™¤æ—§çš„è¯„æµ‹ç»“æœæ–‡ä»¶: {output_file_path}")

                    # 1. ä¸ºå½“å‰æ¨¡å‹è®¾ç½®é‡è¯•å¾ªç¯
                    max_retries = 3
                    retry_count = 0
                    data_to_process = initial_data  # ç¬¬ä¸€æ¬¡è¿è¡Œæ—¶å¤„ç†æ‰€æœ‰æ•°æ®

                    while retry_count < max_retries:
                        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰éœ€è¦å¤„ç†çš„æ•°æ®
                        if not data_to_process or not data_to_process.get('problems'):
                            print("æœ¬è½®æ²¡æœ‰éœ€è¦è¯„æµ‹çš„æ•°æ®ã€‚")
                            break

                        print(f"--- ç¬¬ {retry_count + 1} è½®å°è¯•ï¼šéœ€è¦è¯„æµ‹ {len(data_to_process['problems'])} ä¸ªé¡¹ç›® ---")

                        # 2. æ‰§è¡Œæ¨¡å‹è¯„æµ‹ï¼ˆåªå¯¹éœ€è¦å¤„ç†çš„æ•°æ®ï¼‰
                        newly_processed_results = self.run_model_evaluation(model_instance, data_to_process)

                        # 3. åˆå¹¶ç»“æœå¹¶ä¿å­˜
                        # è¯»å–å·²æœ‰çš„æˆåŠŸç»“æœ
                        _, successful_items = filter_failed_items(output_file_path, response_key="model_response")
                        if successful_items is None:
                            successful_items = []

                        # åˆå¹¶æ–°æ—§ç»“æœ
                        final_results = successful_items + newly_processed_results

                        # æ ¹æ®idå¯¹æœ€ç»ˆç»“æœè¿›è¡Œæ’åº
                        # æ ¹æ®idå¯¹æœ€ç»ˆç»“æœè¿›è¡Œæ’åº
                        if final_results:  # ç¡®ä¿åˆ—è¡¨ä¸ä¸ºç©º
                            # ä½¿ç”¨æˆ‘ä»¬ä¹‹å‰å®šä¹‰çš„ã€æ›´å¥å£®çš„è‡ªç„¶æ’åºé”®
                            def sort_key(item):
                                item_id = item.get('id', '0_0')
                                try:
                                    # å°è¯•æŒ‰ "æ•°å­—_æ•°å­—" æ ¼å¼è§£æ
                                    part1, part2 = map(int, str(item_id).split('_'))
                                    return (part1, part2)
                                except (ValueError, TypeError):
                                    # å¦‚æœå¤±è´¥ï¼Œå°è¯•æŒ‰å•ä¸ªæ•°å­—è§£æ
                                    try:
                                        return (int(item_id), 0)
                                    except (ValueError, TypeError):
                                        # å¦‚æœå†æ¬¡å¤±è´¥ï¼Œåˆ™æ’åœ¨æœ€å
                                        return (float('inf'), float('inf'))

                            final_results = sorted(final_results, key=sort_key)

                        # ä¿å­˜å®Œæ•´ç»“æœ
                        if final_results:
                            self._save_results(final_results, model_version_name, self.task)

                        # 4. æ£€æŸ¥å¤±è´¥é¡¹å¹¶å‡†å¤‡ä¸‹ä¸€è½®
                        failed_items, _ = filter_failed_items(output_file_path, response_key="model_response")

                        if not failed_items:
                            print(f"ğŸ‰ æ¨¡å‹ {model_version_name} æ‰€æœ‰è¯„æµ‹ä»»åŠ¡æˆåŠŸå®Œæˆï¼")
                            break

                        print(f"æ£€æµ‹åˆ° {len(failed_items)} ä¸ªå¤±è´¥çš„é¡¹ç›®ï¼Œå‡†å¤‡ä¸ºæ¨¡å‹ {model_version_name} é‡è¯•ã€‚")

                        # æ›´æ–°ä¸‹ä¸€è½®è¦å¤„ç†çš„æ•°æ®
                        data_to_process = {'problems': failed_items}
                        retry_count += 1

                        if retry_count >= max_retries:
                            print(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ã€‚æ¨¡å‹ {model_version_name} ä»æœ‰ {len(failed_items)} ä¸ªé¡¹ç›®å¤±è´¥ã€‚")

            return {"status": "æ‰€æœ‰æ¨¡å‹çš„è¯„æµ‹å·²å®Œæˆæˆ–è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°"}


def filter_failed_items(output_filepath, response_key="response", error_keywords=["retryerro"]):
    """
    è¯»å–è¾“å‡ºçš„JSONæ–‡ä»¶ï¼Œç­›é€‰å‡ºå¤±è´¥å’ŒæˆåŠŸçš„æ•°æ®é¡¹ã€‚
    :param output_filepath: è¦æ£€æŸ¥çš„è¾“å‡ºæ–‡ä»¶è·¯å¾„ã€‚
    :param response_key: åŒ…å«æ¨¡å‹å“åº”çš„å­—æ®µåï¼ˆä¾‹å¦‚ "response" æˆ– "model_response"ï¼‰ã€‚
    :param error_keywords: åˆ¤æ–­ä¸ºå¤±è´¥çš„å“åº”ä¸­åŒ…å«çš„å…³é”®è¯åˆ—è¡¨ã€‚
    :return: ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«ä¸¤ä¸ªåˆ—è¡¨ (failed_items, successful_items)ã€‚
    """
    if not os.path.exists(output_filepath):
        return None, None

    try:
        with open(output_filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except (json.JSONDecodeError, FileNotFoundError):
        return None, None

    failed_items = []
    successful_items = []

    for item in data:
        # ä½¿ç”¨ä¼ å…¥çš„ response_key æ¥è·å–å†…å®¹
        response = item.get(response_key, "").lower()
        is_failed = False
        if not response:
            is_failed = True
        else:
            for keyword in error_keywords:
                if keyword in response:
                    is_failed = True
                    break

        if is_failed:
            failed_items.append(item)
        else:
            successful_items.append(item)

    return failed_items, successful_items

def run(input_path, output_path, task_name, model_list=None, sample_size=-1, step="UselessCondition", eval_mode=False):
    if eval_mode:
        # è¯„æµ‹æ¨¡å¼
        print("Running in evaluation mode...")
        evaluator = Evaluator(
            output_folder=os.path.dirname(input_path),
            task=task_name,
            output_path=output_path,
            answer_file=input_path,
            eval_mode=True,
            model_list=model_list
        )
        evaluator.evaluate()
        # åœ¨ run å‡½æ•°çš„ else å—ä¸­
    else:
        # æ­£å¸¸æ¨ç†æ¨¡å¼
        max_retries = 10
        retry_count = 0

        output_file_path = os.path.join(output_path, f"{step}.json")
        if os.path.exists(output_file_path):
            os.remove(output_file_path)
            print(f"å·²åˆ é™¤æ—§çš„ç›®æ ‡æ–‡ä»¶: {output_file_path}")

        # åˆå§‹è¦å¤„ç†çš„æ•°æ®ï¼Œå°±æ˜¯è¾“å…¥æ–‡ä»¶çš„å…¨éƒ¨æ•°æ®
        loader = DatasetLoader(input_path, sample_size)
        loader.load_data()
        data_to_process = loader.get_data()

        while retry_count < max_retries:
            print(f"\n--- å¼€å§‹ç¬¬ {retry_count + 1} è½®å°è¯• ---")

            if not data_to_process:
                print("æ²¡æœ‰éœ€è¦å¤„ç†çš„æ•°æ®ï¼Œæµç¨‹ç»“æŸã€‚")
                break

            print(f"æœ¬è½®éœ€è¦å¤„ç† {len(data_to_process)} ä¸ªé¡¹ç›®ã€‚")

            # æ ¸å¿ƒæ¨ç†æµç¨‹ä¿æŒä¸å˜ï¼Œä½†ä½¿ç”¨ data_to_process ä½œä¸ºè¾“å…¥
            task = Task(task_name)

            # æ³¨æ„è¿™é‡Œï¼Œæˆ‘ä»¬å°† data_to_process ä¼ é€’ç»™ Inferrence
            inference = Inferrence(data_to_process, task, input_path, output_path, task_name) # <--- ä¿®æ”¹ç‚¹
            inference.load_models()

            if model_list is None:
                # è¿™ä¸ªå‡½æ•°æˆ‘ä»¬æš‚æ—¶è¿˜æ²¡åŠ¨ï¼Œå¦‚æœä½¿ç”¨éœ€è¦åšç±»ä¼¼ä¿®æ”¹
                inference.run_inference_sequential()
            else:
                inference.run_inference_on_models(model_list, step)

            # --- æ£€æŸ¥ä¸å‡†å¤‡ä¸‹ä¸€è½® ---
            failed_items, successful_items_from_file = filter_failed_items(output_file_path)

            if not failed_items:
                print("ğŸ‰ æ‰€æœ‰ä»»åŠ¡æˆåŠŸå®Œæˆï¼")
                break  # æ²¡æœ‰å¤±è´¥é¡¹ï¼Œé€€å‡ºå¾ªç¯

            print(f"æ£€æµ‹åˆ° {len(failed_items)} ä¸ªå¤±è´¥çš„é¡¹ç›®ï¼Œå‡†å¤‡é‡è¯•ã€‚")

            # å‡†å¤‡ä¸‹ä¸€è½®è¦å¤„ç†çš„æ•°æ®
            data_to_process = failed_items

            retry_count += 1
            if retry_count == max_retries:
                print(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})ã€‚ä»æœ‰ {len(failed_items)} ä¸ªé¡¹ç›®å¤±è´¥ã€‚")

def main():
    parser = argparse.ArgumentParser(description="Run LLM evaluation pipeline.")

    parser.add_argument("--input_path", type=str, default=None, help="The path to the input dataset.")
    parser.add_argument("--output_path", type=str, default=None, help="The path to save the output results.")
    parser.add_argument("--task_name", type=str, default=None, help="The name of the task to be evaluated.")
    parser.add_argument("--model_list", type=str, nargs='*', default=None, help="List of models to run inference on.")  # ä½¿ç”¨ nargs='*' æ¥æ”¶å¯é€‰çš„å¤šä¸ªæ¨¡å‹åç§°åˆ—è¡¨
    parser.add_argument("--sample_size", type=int, default=-1, help="The number of sample to be evaluated.")  # éšæœºé€‰æ‹©æ•°æ®é›†ä¸­çš„éƒ¨åˆ†æ ·æœ¬è¿›è¡Œè¯„æµ‹ï¼Œé»˜è®¤å…¨éƒ¨è¯„æµ‹
    parser.add_argument("--step", type=str, default=None, help="This choose the step of the frame.")
    parser.add_argument("--eval", action="store_true", default=False, help="Run in evaluation mode.")
    args = parser.parse_args()

    if args.eval:
        if args.input_path is None:
            args.input_path = "./dataset/9_CombinedProblems/CombinedProblems.json"
        if args.output_path is None:
            args.output_path = "./evaluation_results"
        args.task_name = "Evaluate"
        if args.model_list is None:
            # æµ‹è¯•ï¼ˆå…è´¹apiï¼‰
            # args.model_list = ["Llama"]
            # å®Œæ•´
            # args.model_list = ["Doubao"]
            # args.model_list = ["Qwen"]
            args.model_list = ["Openai", "Qwen", "Moonshot", "Gemini", "Deepseek"]
            # args.model_list = ["Openai", "Qwen", "Gemini", "Deepseek"]
            # args.model_list = ["Qwen", "Moonshot", "Deepseek"]
            # args.model_list = ["Moonshot"]
            # args.model_list = ["Moonshot"]
            # args.model_list = ["Moonshot"]
            # args.model_list = ["Qwen", "Zhipu", "Doubao", "Deepseek"]
            # args.model_list = ["Qwen", "Llama", "Zhipu", "Doubao", "Deepseek"]

    if args.step == "UselessCondition":# äº§å‡ºçš„æ–‡ä»¶åç§°
        if args.input_path is None:
            args.input_path = "./dataset/0_InitialData/InitialData.json"
        if args.output_path is None:
            args.output_path = "./dataset/1_UselessCondition"
        args.task_name = "UselessCondition"# è¯»å–çš„æ–‡ä»¶åç§°
        if args.model_list is None:
            args.model_list = ["Openai"]

    elif args.step == "ConfusedCondition":
        if args.input_path is None:
            args.input_path = "./dataset/1_UselessCondition/UselessCondition.json"
        if args.output_path is None:
            args.output_path = "./dataset/2_ConfusedCondition"
        args.task_name = "ConfusedCondition"
        if args.model_list is None:
            args.model_list = ["Openai"]

    elif args.step == "FormulaClarifier":
        if args.input_path is None:
            args.input_path = "./dataset/3_Translated/Translated.json"
        if args.output_path is None:
            args.output_path = "./dataset/4_FormulaClarifier"
        args.task_name = "FormulaClarifier"
        if args.model_list is None:
            args.model_list = ["Deepseek"]

    elif args.step == "MisleadingCondition":
        if args.input_path is None:
            args.input_path = "./dataset/4_FormulaClarifier/FormulaClarifier.json"
        if args.output_path is None:
            args.output_path = "./dataset/5_MisleadingCondition"
        args.task_name = "MisleadingCondition"
        if args.model_list is None:
            args.model_list = ["Openai"] # Deepseek

    elif args.step == "ContextGen":
        if args.input_path is None:
            args.input_path = "./dataset/5_MisleadingCondition/MisleadingCondition.json"
        if args.output_path is None:
            args.output_path = "./dataset/6_ContextGen"
        args.task_name = "ContextGen"
        if args.model_list is None:
            args.model_list = ["Openai"]

    elif args.step == "AddCondition":
        if args.input_path is None:
            args.input_path = "./dataset/6_ContextGen/ContextGen.json"
        if args.output_path is None:
            args.output_path = "./dataset/7_AddCondition"
        args.task_name = "AddCondition"
        if args.model_list is None:
            args.model_list = ["Openai"]
    else:
        # é»˜è®¤å€¼æˆ–å…¶ä»–æƒ…å†µ
        args.task_name = "DefaultTask"
    # å˜å¼‚
    print(args.input_path)

    # Call run function
    run(
        args.input_path,
        args.output_path,
        args.task_name,
        args.model_list,
        args.sample_size,
        args.step,
        eval_mode=args.eval
    )

if __name__ == "__main__":
    main()
