import json
import re
# ç¡®ä¿ä½ æœ‰åä¸º config.py çš„é…ç½®æ–‡ä»¶
import model.config as config
# è¿™ä¸ªå‡½æ•°éœ€è¦è¢«å®šä¹‰ï¼Œæˆ‘ä»¬å°†åœ¨æµ‹è¯•ä¸­åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿï¼ˆmockï¼‰ç‰ˆæœ¬
from model.base_model_api import generation_result
import argparse
import os # <--- æ·»åŠ  os åº“ï¼Œç”¨äºæ–‡ä»¶æ£€æŸ¥
import concurrent.futures # <--- æ·»åŠ å¹¶å‘åº“


def filter_failed_items(filepath):
    """
    è¯»å–æ–‡ä»¶ï¼Œç­›é€‰å‡ºå¤±è´¥å’ŒæˆåŠŸçš„æ•°æ®é¡¹ã€‚
    å¤±è´¥é¡¹ï¼šæ²¡æœ‰ 'difficulty_score' é”®ï¼Œæˆ–è€…è¯¥é”®çš„å€¼ä¸æ˜¯æ•°å­—ã€‚
    æˆåŠŸé¡¹ï¼šæ‹¥æœ‰ 'difficulty_score' é”®ä¸”å€¼ä¸ºæ•°å­—ã€‚
    :param filepath: è¦æ£€æŸ¥çš„ JSON æ–‡ä»¶è·¯å¾„ã€‚
    :return: ä¸€ä¸ªå…ƒç»„ (failed_items, successful_items)ã€‚
    """
    if not os.path.exists(filepath):
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯´æ˜æ˜¯ç¬¬ä¸€æ¬¡è¿è¡Œï¼Œæ‰€æœ‰é¡¹ç›®éƒ½åº”è¢«å¤„ç†
        try:
            with open(filepath.replace('.json', '_initial.json'), 'r', encoding='utf-8') as f:
                return json.load(f), []
        except FileNotFoundError:
            # å¦‚æœåˆå§‹æ–‡ä»¶ä¹Ÿä¸å­˜åœ¨ï¼Œæˆ‘ä»¬éœ€è¦ä»ä¸€ä¸ªå¹²å‡€çš„æºå¤´åŠ è½½
            # ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬å‡è®¾ evaluator ä¼šå¤„ç†è¿™ç§æƒ…å†µ
            return [], []

    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except (json.JSONDecodeError, FileNotFoundError):
        return [], []  # å¦‚æœæ–‡ä»¶ä¸ºç©ºæˆ–æŸåï¼Œè¿”å›ç©ºåˆ—è¡¨

    failed_items = []
    successful_items = []

    for item in data:
        score = item.get('difficulty_score')
        # å¦‚æœ'difficulty_score'ä¸å­˜åœ¨ï¼Œæˆ–è€…å­˜åœ¨ä½†ä¸æ˜¯æ•´æ•°æˆ–æµ®ç‚¹æ•°ï¼Œåˆ™è§†ä¸ºå¤±è´¥
        if score is None or not isinstance(score, (int, float)):
            failed_items.append(item)
        else:
            successful_items.append(item)

    return failed_items, successful_items

class Evaluator:
    def __init__(self, file_to_process):
        """
        åˆå§‹åŒ– Evaluator ç±»
        :param output_file: éœ€è¦è¯„ä¼°çš„è¾“å‡ºæ–‡ä»¶è·¯å¾„
        :param answer_file: å­˜æ”¾æ ‡å‡†ç­”æ¡ˆçš„ JSON æ–‡ä»¶è·¯å¾„,æ­¤è¯„ä¼°æ²¡æœ‰æ ‡å‡†ç­”æ¡ˆ
        """
        self.file_path = file_to_process # å°† output_file é‡å‘½åä¸ºæ›´é€šç”¨çš„ file_path
        # ... ç±»çš„å…¶ä»–åˆå§‹åŒ–ä»£ç ä¿æŒä¸å˜ ...
        self.model_class = {}
        self.model_instance = {}
        self.failed_models = []
        self.prompts = []

        self.model_class = {}  # å­˜å‚¨æ¨¡å‹ç±»
        self.model_instance = {}  # å­˜å‚¨æ¨¡å‹å®ä¾‹
        self.failed_models = []  # åˆå§‹åŒ–å¤±è´¥æ¨¡å‹åˆ—è¡¨

        self.prompts = []

        # åªå¯¼å…¥å’Œå®ä¾‹åŒ– Openai æ¨¡å‹
        model_name = "Openai"
        # è¿™é‡Œçš„è·¯å¾„æŒ‡çš„æ˜¯ Python æ–‡ä»¶åï¼Œä¾‹å¦‚ 'openai_api.py'
        model_path = "openai_api"

        # åŠ¨æ€å¯¼å…¥ Openai æ¨¡å‹ç±»
        try:
            execute_command = f"from model.{model_path} import {model_name}\n" \
                              f"self.model_class[\"{model_name}\"] = {model_name}"
            exec(execute_command)
            print(f"æˆåŠŸåŠ è½½æ¨¡å‹ç±»: {model_name}")
        except Exception as e:
            print(f"åŠ è½½æ¨¡å‹ {model_name} æ—¶å‡ºé”™: {e}")

        # å®ä¾‹åŒ– Openai æ¨¡å‹å¯¹è±¡
        try:
            # å‡è®¾ config.py æ–‡ä»¶ä¸­æœ‰ä¸€ä¸ªç±»ä¼¼è¿™æ ·çš„å˜é‡: openai_api_keys = ["sk-xxxx", "sk-yyyy"]
            api_keys = getattr(config, f"{model_name.lower()}_api_keys", None)
            if not api_keys:
                raise ValueError(f"åœ¨ config.py ä¸­æœªæ‰¾åˆ° {model_name} çš„ API å¯†é’¥ã€‚")

            # å‡è®¾ Openai ç±»ä¸­å®šä¹‰äº†æ¨èæ¨¡å‹çš„åˆ—è¡¨
            model_version = self.model_class[model_name].MOST_RECOMMENDED_MODEL[0]
            self.model_instance = self.model_class[model_name](api_keys, model_version)
            print(f"æˆåŠŸå®ä¾‹åŒ–æ¨¡å‹: {model_name}ï¼Œä½¿ç”¨ç‰ˆæœ¬: {model_version}")
        except Exception as e:
            print(f"å®ä¾‹åŒ–æ¨¡å‹ {model_name} æ—¶å‡ºé”™: {e}")
            self.failed_models.append(model_name)

    def _process_chunk(self, data_chunk, prompts_chunk):
        """
        å¤„ç†å•ä¸ªæ•°æ®å—ï¼šè°ƒç”¨APIï¼Œå¤„ç†è¿”å›çš„åˆ†æ•°ï¼Œå¹¶è¿”å›é™„å¸¦ç»Ÿè®¡ä¿¡æ¯çš„ç»“æœã€‚
        """
        processed_chunk = []
        prompt_tokens = 0
        completion_tokens = 0
        successful_calls = 0

        # è°ƒç”¨æ¨¡å‹API
        results = generation_result(self.model_instance, prompts_chunk)

        for i, item in enumerate(data_chunk):
            if i < len(results):
                status, score_value, full_json_response = results[i]

                if status == "success":
                    usage_data = full_json_response.get('usage', {})
                    prompt_tokens += usage_data.get('prompt_tokens', 0)
                    completion_tokens += usage_data.get('completion_tokens', 0)
                    successful_calls += 1

                # Evaluator ç±»ä¸­çš„ _process_chunk æ–¹æ³•

                try:
                    # ä½¿ç”¨ float() æ¥æ­£ç¡®å¤„ç†å¯èƒ½å¸¦å°æ•°çš„è¯„åˆ†
                    final_score = float(score_value)
                except (ValueError, TypeError):
                    final_score = str(score_value)

                # æ›´æ–°é¡¹ç›®å­—å…¸ï¼Œä½†ä¸ä¿®æ”¹åŸå§‹çš„ item
                processed_item = {**item, 'difficulty_score': final_score}
                processed_chunk.append(processed_item)

        return processed_chunk, prompt_tokens, completion_tokens, successful_calls

    def _run_parallel_evaluation(self, items_to_process, prompts):
        """
        ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†æ‰€æœ‰éœ€è¦è¯„ä¼°çš„é¡¹ç›®ã€‚
        """
        all_processed_data = []
        total_prompt_tokens = 0
        total_completion_tokens = 0
        total_successful_calls = 0
        NUM_WORKERS = 20  # ä½ å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´çº¿ç¨‹æ•°

        # å°†æ•°æ®å’Œæç¤ºåˆ†å‰²æˆå—
        chunk_size = (len(items_to_process) + NUM_WORKERS - 1) // NUM_WORKERS
        data_chunks = [items_to_process[i:i + chunk_size] for i in range(0, len(items_to_process), chunk_size)]
        prompts_chunks = [prompts[i:i + chunk_size] for i in range(0, len(prompts), chunk_size)]

        with concurrent.futures.ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:
            future_to_chunk = {
                executor.submit(self._process_chunk, data_chunks[i], prompts_chunks[i]): i
                for i in range(len(data_chunks))
            }

            print(f"ä»»åŠ¡å·²åˆ†å‘ç»™ {NUM_WORKERS} ä¸ªå·¥ä½œçº¿ç¨‹...")
            for future in concurrent.futures.as_completed(future_to_chunk):
                try:
                    processed_chunk, p_tokens, c_tokens, s_calls = future.result()
                    all_processed_data.extend(processed_chunk)
                    total_prompt_tokens += p_tokens
                    total_completion_tokens += c_tokens
                    total_successful_calls += s_calls
                except Exception as exc:
                    print(f"ä¸€ä¸ªå¤„ç†å—äº§ç”Ÿå¼‚å¸¸: {exc}")

        return all_processed_data, total_prompt_tokens, total_completion_tokens, total_successful_calls

    def evaluate(self):
        """
        è¯„ä¼°ä¸»æµç¨‹ï¼ŒåŒ…å«é‡è¯•å’Œå¹¶è¡Œå¤„ç†æœºåˆ¶ã€‚
        """
        max_retries = 5
        retry_count = 0

        # ä¸ºäº†é˜²æ­¢åœ¨é‡è¯•è¿‡ç¨‹ä¸­åå¤è¯»å–å’Œè¦†ç›–æºæ–‡ä»¶ï¼Œæˆ‘ä»¬å…ˆåšä¸€ä¸ªå¤‡ä»½
        # å¦‚æœè¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨ï¼Œæˆ‘ä»¬å‡è®¾å®ƒå°±æ˜¯æˆ‘ä»¬è¦å¤„ç†çš„çŠ¶æ€
        if not os.path.exists(self.file_path):
            # å°è¯•ä»ä¸€ä¸ªé¢„å®šä¹‰çš„åˆå§‹æ–‡ä»¶åŠ è½½æ•°æ®
            initial_file = 'dataset/8_CrossedCondition/CrossedCondition.json'  # ç¡®ä¿è¿™ä¸ªè·¯å¾„æ­£ç¡®
            try:
                with open(initial_file, 'r', encoding='utf-8') as f:
                    initial_data = json.load(f)
                with open(self.file_path, 'w', encoding='utf-8') as f:
                    json.dump(initial_data, f, ensure_ascii=False, indent=4)
                print(f"ä» {initial_file} åˆ›å»ºäº†å·¥ä½œæ–‡ä»¶ {self.file_path}")
            except FileNotFoundError:
                print(f"é”™è¯¯: åˆå§‹æ–‡ä»¶ {initial_file} æœªæ‰¾åˆ°ï¼Œæ— æ³•å¼€å§‹å¤„ç†ã€‚")
                return

        while retry_count < max_retries:
            print(f"\n--- å¼€å§‹ç¬¬ {retry_count + 1}/{max_retries} è½®å°è¯• ---")

            # 1. ç­›é€‰å‡ºéœ€è¦å¤„ç†çš„å¤±è´¥é¡¹å’Œå·²ç»æˆåŠŸçš„é¡¹ç›®
            items_to_process, successful_items = filter_failed_items(self.file_path)

            if not items_to_process:
                print("ğŸ‰ æ‰€æœ‰é¡¹ç›®å‡å·²æˆåŠŸå¤„ç†ï¼æµç¨‹ç»“æŸã€‚")
                break

            print(f"æ‰¾åˆ° {len(items_to_process)} ä¸ªéœ€è¦å¤„ç†çš„é¡¹ç›®ã€‚")

            # 2. ä»…ä¸ºéœ€è¦å¤„ç†çš„é¡¹ç›®ç”Ÿæˆ Prompts
            prompts_to_run = []
            for item in items_to_process:
                question = item.get('prompt', '')
                prompt = (
                    "Rate the cognitive difficulty of the following problem on a scale of 0-10.\n\n"
                    "IMPORTANT: The underlying math is simple. The real difficulty is in the confusing language, irrelevant details, and logical steps. Your score must reflect how hard the text is to understand, NOT how hard the math is.\n\n"
                    "Use this scale:\n"
                    "- 0-3: Language is direct and easy to follow.\n"
                    "- 4-6: Language is complex or contains noise, requiring careful reading.\n"
                    "- 7-10: Language is intentionally confusing, tricky, or hides the core question.\n\n"
                    "Problem:\n"
                    f"<question>{question}</question>\n\n"
                    "Your response must be only a single numerical score. Decimal values (e.g., 4.5, 7.2) are encouraged for finer granularity."
                )
                prompts_to_run.append(prompt)

            # 3. è°ƒç”¨å¹¶è¡Œè°ƒåº¦å™¨æ‰§è¡Œå¤„ç†
            (newly_processed_data, p_tokens,
             c_tokens, s_calls) = self._run_parallel_evaluation(items_to_process, prompts_to_run)

            print("\n--- æœ¬è½®Tokenä½¿ç”¨ç»Ÿè®¡ ---")
            print(f"æˆåŠŸçš„APIè°ƒç”¨æ¬¡æ•°: {s_calls} / {len(prompts_to_run)}")
            print(f"è¾“å…¥ Tokens: {p_tokens}, è¾“å‡º Tokens: {c_tokens}")
            print("--------------------------\n")

            # 4. åˆå¹¶æ–°å¤„ç†å¥½çš„æ•°æ®å’Œä¹‹å‰å·²ç»æˆåŠŸçš„æ•°æ®
            final_output_data = successful_items + newly_processed_data

            # 5. (é‡è¦) å¯¹åˆå¹¶åçš„æ•°æ®æŒ‰ id æ’åºï¼Œç¡®ä¿æ–‡ä»¶å†…å®¹é¡ºåºç¨³å®š
            if final_output_data:
                def sort_key(item):
                    # è·å–IDï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨ '0_0' ä½œä¸ºé»˜è®¤å€¼
                    item_id = item.get('id', '0_0')
                    try:
                        # åˆ†å‰²IDå¹¶è½¬æ¢ä¸ºæ•´æ•°å…ƒç»„ï¼Œä¾‹å¦‚ "10_2" -> (10, 2)
                        part1, part2 = map(int, str(item_id).split('_'))
                        return (part1, part2)
                    except (ValueError, TypeError):
                        # å¦‚æœIDæ ¼å¼ä¸æ­£ç¡®ï¼Œåˆ™è¿”å›ä¸€ä¸ªé»˜è®¤å€¼ä½¿å…¶æ’åœ¨åé¢
                        return (float('inf'), float('inf'))

                final_output_data = sorted(final_output_data, key=sort_key)

            # 6. å°†å®Œæ•´æ•°æ®å†™å›æ–‡ä»¶ï¼Œä¸ºä¸‹ä¸€è½®æˆ–æœ€ç»ˆç»“æœåšå‡†å¤‡
            with open(self.file_path, 'w', encoding='utf-8') as f:
                json.dump(final_output_data, f, ensure_ascii=False, indent=4)

            print(
                f"ç¬¬ {retry_count + 1} è½®å®Œæˆï¼Œç»“æœå·²ä¿å­˜ã€‚æ–‡ä»¶ '{self.file_path}' åŒ…å« {len(final_output_data)} ä¸ªé¡¹ç›®ã€‚")

            retry_count += 1
            if retry_count == max_retries:
                remaining_failures, _ = filter_failed_items(self.file_path)
                if remaining_failures:
                    print(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ã€‚ä»æœ‰ {len(remaining_failures)} ä¸ªé¡¹ç›®å¤„ç†å¤±è´¥ã€‚")
                else:
                    print("ğŸ‰ æ‰€æœ‰é¡¹ç›®å‡å·²åœ¨æœ€åä¸€æ¬¡å°è¯•ä¸­æˆåŠŸå¤„ç†ï¼")

        print("\nè¯„ä¼°æµç¨‹æ‰§è¡Œå®Œæ¯•ã€‚")


# ==============================================================================
# ç”¨äºæµ‹è¯• Evaluator çš„ä¸»æ‰§è¡Œä»£ç å—
# ==============================================================================
if __name__ == "__main__":
    # 1. åˆ›å»ºä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(description="Add difficulty scores to a dataset using an LLM.")

    # 2. å®šä¹‰ç¨‹åºå¯ä»¥æ¥æ”¶çš„ --file_path å‚æ•°ï¼Œå¹¶è®¾ç½®é»˜è®¤å€¼
    default_file_path = 'dataset/8_CrossedCondition/CrossedCondition.json'
    parser.add_argument("--file_path", type=str, default=default_file_path,
                        help="Path to the JSON file to be processed.")

    # 3. è§£æç”¨æˆ·ä»å‘½ä»¤è¡Œè¾“å…¥çš„å‚æ•°
    args = parser.parse_args()

    # 4. ä½¿ç”¨è§£æåˆ°çš„å‚æ•°åˆ›å»ºå¹¶å®ä¾‹åŒ– Evaluator
    print("æ­£åœ¨åˆå§‹åŒ–è¯„ä¼°å™¨...")
    print(f"å°†è¦å¤„ç†çš„æ–‡ä»¶: {args.file_path}")
    evaluator = Evaluator(file_to_process=args.file_path) # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°

    # 5. è¿è¡Œè¯„ä¼°
    if not evaluator.failed_models:
        evaluator.evaluate()
    else:
        print("ç”±äºæ¨¡å‹åˆå§‹åŒ–å¤±è´¥ï¼Œè¯„ä¼°å·²è·³è¿‡ã€‚")